# Worker Node æ¨¡çµ„æ–‡æª”

## ğŸ“‹ æ¦‚è¿°

Worker Node æ˜¯ HiveMind åˆ†æ•£å¼è¨ˆç®—å¹³å°çš„è¨ˆç®—åŸ·è¡Œç¯€é»ï¼Œè² è²¬æ¥æ”¶å’ŒåŸ·è¡Œä¾†è‡ª Master Node åˆ†é…çš„è¨ˆç®—ä»»å‹™ï¼Œä¸¦å°‡çµæœå›å‚³çµ¦ç³»çµ±ã€‚

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Worker Node      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Task Executor     â”‚
â”‚ â€¢ Resource Monitor  â”‚
â”‚ â€¢ Result Handler    â”‚
â”‚ â€¢ Status Reporter   â”‚
â”‚ â€¢ Docker Engine     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€ gRPC Client (to Node Pool)
        â”œâ”€ Docker Containers
        â”œâ”€ Local Storage
        â””â”€ System Resources
```

## ğŸ”§ æ ¸å¿ƒçµ„ä»¶

### 1. Worker Node Main (`worker_node.py`)
- **åŠŸèƒ½**: ä¸»è¦å·¥ä½œç¯€é»æœå‹™
- **å”è­°**: gRPC Client to Node Pool
- **å®¹å™¨**: Docker åŸºç¤ä»»å‹™åŸ·è¡Œç’°å¢ƒ

**ä¸»è¦åŠŸèƒ½**:
```python
class WorkerNode:
    def __init__(self, node_id, node_pool_address):
        self.node_id = node_id
        self.node_pool_client = NodePoolClient(node_pool_address)
        self.task_executor = TaskExecutor()
        self.resource_monitor = ResourceMonitor()
        
    def start(self):
        """å•Ÿå‹•å·¥ä½œç¯€é»"""
        self.register_with_node_pool()
        self.start_heartbeat()
        self.start_task_polling()
        
    def register_with_node_pool(self):
        """å‘ Node Pool è¨»å†Šç¯€é»"""
        
    def execute_task(self, task):
        """åŸ·è¡Œåˆ†é…çš„ä»»å‹™"""
        
    def report_result(self, task_id, result):
        """å›å ±ä»»å‹™åŸ·è¡Œçµæœ"""
```

### 2. Task Executor (`task_executor.py`)
- **åŠŸèƒ½**: ä»»å‹™åŸ·è¡Œå¼•æ“
- **æ”¯æ´**: Docker å®¹å™¨åŒ–åŸ·è¡Œ
- **éš”é›¢**: é€²ç¨‹å’Œè³‡æºéš”é›¢

### 3. Resource Monitor (`resource_monitor.py`)
- **åŠŸèƒ½**: ç³»çµ±è³‡æºç›£æ§
- **æŒ‡æ¨™**: CPUã€è¨˜æ†¶é«”ã€ç£ç¢Ÿã€ç¶²è·¯
- **å ±å‘Š**: å¯¦æ™‚è³‡æºä½¿ç”¨æƒ…æ³

### 4. Docker Manager (`docker_manager.py`)
- **åŠŸèƒ½**: Docker å®¹å™¨ç®¡ç†
- **è·è²¬**: å®¹å™¨å‰µå»ºã€åŸ·è¡Œã€æ¸…ç†
- **å®‰å…¨**: å®¹å™¨å®‰å…¨é…ç½®

## ğŸ—‚ï¸ æ–‡ä»¶çµæ§‹

```
worker/
â”œâ”€â”€ worker_node.py              # ä¸»è¦å·¥ä½œç¯€é»æœå‹™
â”œâ”€â”€ task_executor.py           # ä»»å‹™åŸ·è¡Œå¼•æ“
â”œâ”€â”€ resource_monitor.py        # è³‡æºç›£æ§å™¨
â”œâ”€â”€ docker_manager.py          # Docker å®¹å™¨ç®¡ç†
â”œâ”€â”€ nodepool_pb2.py           # Protocol Buffer æ–‡ä»¶
â”œâ”€â”€ nodepool_pb2_grpc.py      # gRPC å®¢æˆ¶ç«¯æ–‡ä»¶
â”œâ”€â”€ nodepool.proto            # Protocol Buffer å®šç¾©
â”œâ”€â”€ requirements.txt          # Python ä¾è³´åŒ…
â”œâ”€â”€ Dockerfile               # Docker é¡åƒæ§‹å»ºæ–‡ä»¶
â”œâ”€â”€ run_task.sh             # ä»»å‹™åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ build.py                # æ§‹å»ºè…³æœ¬
â”œâ”€â”€ make.py                 # ç·¨è­¯è…³æœ¬
â”œâ”€â”€ file.ico                # æ‡‰ç”¨åœ–æ¨™
â”œâ”€â”€ README.md               # æ¨¡çµ„èªªæ˜æ–‡æª”
â”œâ”€â”€ README.en.md            # è‹±æ–‡èªªæ˜æ–‡æª”
â”œâ”€â”€ static/                 # éœæ…‹è³‡æº
â”œâ”€â”€ templates/              # HTML æ¨¡æ¿
â”œâ”€â”€ hivemind_worker/        # Worker æ‰“åŒ…é …ç›®
â””â”€â”€ HiveMind-Worker-Release/ # ç™¼å¸ƒåŒ…
    â”œâ”€â”€ install.sh          # å®‰è£è…³æœ¬
    â”œâ”€â”€ start_worker.cmd    # Windows å•Ÿå‹•è…³æœ¬
    â””â”€â”€ ...
```

## ğŸš€ éƒ¨ç½²å’Œé…ç½®

### æœ¬åœ°é–‹ç™¼ç’°å¢ƒ
```bash
cd worker
pip install -r requirements.txt
python worker_node.py --node-id=worker-001 --node-pool=localhost:50051
```

### Docker å®¹å™¨éƒ¨ç½²
```bash
# æ§‹å»º Docker é¡åƒ
docker build -t hivemind-worker .

# é‹è¡Œ Worker å®¹å™¨
docker run -d \
  --name hivemind-worker-001 \
  -e NODE_ID=worker-001 \
  -e NODE_POOL_ADDRESS=host.docker.internal:50051 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  hivemind-worker
```

### é…ç½®æ–‡ä»¶ç¯„ä¾‹
```python
# config.py
import os

class WorkerConfig:
    # ç¯€é»é…ç½®
    NODE_ID = os.environ.get('NODE_ID') or f'worker-{uuid.uuid4().hex[:8]}'
    NODE_POOL_ADDRESS = os.environ.get('NODE_POOL_ADDRESS') or 'localhost:50051'
    
    # è³‡æºé™åˆ¶
    MAX_CPU_CORES = int(os.environ.get('MAX_CPU_CORES') or 0)  # 0 = ç„¡é™åˆ¶
    MAX_MEMORY_GB = float(os.environ.get('MAX_MEMORY_GB') or 0)  # 0 = ç„¡é™åˆ¶
    MAX_DISK_GB = float(os.environ.get('MAX_DISK_GB') or 0)    # 0 = ç„¡é™åˆ¶
    
    # ä»»å‹™é…ç½®
    TASK_TIMEOUT = int(os.environ.get('TASK_TIMEOUT') or 3600)  # 1å°æ™‚
    MAX_CONCURRENT_TASKS = int(os.environ.get('MAX_CONCURRENT_TASKS') or 1)
    
    # å¿ƒè·³é…ç½®
    HEARTBEAT_INTERVAL = int(os.environ.get('HEARTBEAT_INTERVAL') or 30)  # 30ç§’
    
    # Docker é…ç½®
    DOCKER_ENABLED = os.environ.get('DOCKER_ENABLED', 'true').lower() == 'true'
    DOCKER_NETWORK = os.environ.get('DOCKER_NETWORK') or 'hivemind'
    
    # å­˜å„²é…ç½®
    WORK_DIR = os.environ.get('WORK_DIR') or './work'
    TEMP_DIR = os.environ.get('TEMP_DIR') or './temp'
    LOG_DIR = os.environ.get('LOG_DIR') or './logs'
```

## ğŸ“¡ èˆ‡ Node Pool é€šä¿¡

### gRPC å®¢æˆ¶ç«¯å¯¦ç¾
```python
import grpc
import time
from concurrent import futures
from worker import nodepool_pb2, nodepool_pb2_grpc

class NodePoolClient:
    def __init__(self, address):
        self.address = address
        self.channel = grpc.insecure_channel(address)
        self.stub = nodepool_pb2_grpc.NodeManagerStub(self.channel)
        
    def register_node(self, node_info):
        """è¨»å†Šç¯€é»åˆ° Node Pool"""
        request = nodepool_pb2.RegisterNodeRequest(
            node_id=node_info['node_id'],
            ip_address=node_info['ip_address'],
            port=node_info['port'],
            cpu_cores=node_info['cpu_cores'],
            memory_gb=node_info['memory_gb'],
            disk_gb=node_info['disk_gb'],
            capabilities=node_info.get('capabilities', [])
        )
        
        try:
            response = self.stub.RegisterNode(request)
            return response.success
        except grpc.RpcError as e:
            print(f"è¨»å†Šç¯€é»å¤±æ•—: {e}")
            return False
    
    def send_heartbeat(self, node_id, status):
        """ç™¼é€å¿ƒè·³ä¿¡è™Ÿ"""
        request = nodepool_pb2.UpdateNodeStatusRequest(
            node_id=node_id,
            status=status,
            timestamp=int(time.time())
        )
        
        try:
            response = self.stub.UpdateNodeStatus(request)
            return response.success
        except grpc.RpcError as e:
            print(f"å¿ƒè·³ç™¼é€å¤±æ•—: {e}")
            return False
    
    def get_assigned_tasks(self, node_id):
        """ç²å–åˆ†é…çµ¦æ­¤ç¯€é»çš„ä»»å‹™"""
        request = nodepool_pb2.GetAssignedTasksRequest(node_id=node_id)
        
        try:
            response = self.stub.GetAssignedTasks(request)
            return response.tasks
        except grpc.RpcError as e:
            print(f"ç²å–ä»»å‹™å¤±æ•—: {e}")
            return []
    
    def report_task_result(self, task_id, result, status):
        """å›å ±ä»»å‹™åŸ·è¡Œçµæœ"""
        request = nodepool_pb2.ReportTaskResultRequest(
            task_id=task_id,
            node_id=self.node_id,
            result=result,
            status=status,
            timestamp=int(time.time())
        )
        
        try:
            response = self.stub.ReportTaskResult(request)
            return response.success
        except grpc.RpcError as e:
            print(f"çµæœå›å ±å¤±æ•—: {e}")
            return False
```

## ğŸ³ Docker ä»»å‹™åŸ·è¡Œ

### Docker å®¹å™¨ç®¡ç†
```python
import docker
import json
import tempfile
import os

class DockerTaskExecutor:
    def __init__(self):
        self.client = docker.from_env()
        self.network_name = 'hivemind'
        self.ensure_network_exists()
    
    def ensure_network_exists(self):
        """ç¢ºä¿ HiveMind ç¶²è·¯å­˜åœ¨"""
        try:
            self.client.networks.get(self.network_name)
        except docker.errors.NotFound:
            self.client.networks.create(
                self.network_name,
                driver="bridge",
                options={"com.docker.network.bridge.enable_icc": "true"}
            )
    
    def execute_task(self, task):
        """åœ¨ Docker å®¹å™¨ä¸­åŸ·è¡Œä»»å‹™"""
        container_name = f"hivemind-task-{task['id']}"
        
        # æº–å‚™ä»»å‹™æ•¸æ“š
        work_dir = tempfile.mkdtemp(prefix=f"task-{task['id']}-")
        self.prepare_task_data(task, work_dir)
        
        try:
            # å‰µå»ºä¸¦å•Ÿå‹•å®¹å™¨
            container = self.client.containers.run(
                image=task.get('docker_image', 'python:3.9-slim'),
                command=task['command'],
                name=container_name,
                network=self.network_name,
                volumes={
                    work_dir: {'bind': '/workspace', 'mode': 'rw'}
                },
                working_dir='/workspace',
                mem_limit=task.get('memory_limit', '1g'),
                cpu_quota=task.get('cpu_quota', 100000),  # 1 CPU core
                detach=True,
                remove=False  # ä¿ç•™å®¹å™¨ä»¥ç²å–çµæœ
            )
            
            # ç­‰å¾…å®¹å™¨å®Œæˆ
            result = container.wait(timeout=task.get('timeout', 3600))
            
            # ç²å–è¼¸å‡º
            logs = container.logs(stdout=True, stderr=True).decode('utf-8')
            
            # ç²å–çµæœæ–‡ä»¶
            result_data = self.collect_results(container, work_dir)
            
            # æ¸…ç†å®¹å™¨
            container.remove()
            
            return {
                'status': 'completed' if result['StatusCode'] == 0 else 'failed',
                'exit_code': result['StatusCode'],
                'logs': logs,
                'result_data': result_data
            }
            
        except docker.errors.ContainerError as e:
            return {
                'status': 'failed',
                'error': str(e),
                'exit_code': e.exit_status
            }
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e)
            }
        finally:
            # æ¸…ç†å·¥ä½œç›®éŒ„
            self.cleanup_work_dir(work_dir)
    
    def prepare_task_data(self, task, work_dir):
        """æº–å‚™ä»»å‹™åŸ·è¡Œæ‰€éœ€çš„æ•¸æ“šæ–‡ä»¶"""
        # å¯«å…¥ä»»å‹™é…ç½®
        with open(os.path.join(work_dir, 'task.json'), 'w') as f:
            json.dump(task, f, indent=2)
        
        # å¯«å…¥ä»»å‹™æ•¸æ“šæ–‡ä»¶
        if 'input_files' in task:
            for filename, content in task['input_files'].items():
                file_path = os.path.join(work_dir, filename)
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                
                if isinstance(content, str):
                    with open(file_path, 'w') as f:
                        f.write(content)
                else:
                    with open(file_path, 'wb') as f:
                        f.write(content)
    
    def collect_results(self, container, work_dir):
        """æ”¶é›†ä»»å‹™åŸ·è¡Œçµæœ"""
        result_data = {}
        
        # å¾å®¹å™¨è¤‡è£½çµæœæ–‡ä»¶
        try:
            # ç²å–çµæœç›®éŒ„å…§å®¹
            archive, _ = container.get_archive('/workspace/results')
            
            # è§£å£“ç¸®ä¸¦è®€å–æ–‡ä»¶
            import tarfile
            import io
            
            tar = tarfile.open(fileobj=io.BytesIO(archive.read()))
            for member in tar.getmembers():
                if member.isfile():
                    file_data = tar.extractfile(member).read()
                    result_data[member.name] = file_data
            
        except docker.errors.NotFound:
            # æ²’æœ‰çµæœç›®éŒ„ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰çµæœæ–‡ä»¶
            pass
        
        return result_data
```

### ä»»å‹™é¡å‹æ”¯æ´

#### Python è¨ˆç®—ä»»å‹™
```python
def execute_python_task(self, task):
    """åŸ·è¡Œ Python è¨ˆç®—ä»»å‹™"""
    python_code = task['code']
    requirements = task.get('requirements', [])
    
    # å‰µå»º Dockerfile
    dockerfile_content = f"""
FROM python:3.9-slim

# å®‰è£ä¾è³´åŒ…
RUN pip install {' '.join(requirements)}

# è¤‡è£½ä»»å‹™ä»£ç¢¼
COPY task.py /app/task.py
WORKDIR /app

# åŸ·è¡Œä»»å‹™
CMD ["python", "task.py"]
"""
    
    return self.execute_custom_docker_task(task, dockerfile_content, {'task.py': python_code})
```

#### æ©Ÿå™¨å­¸ç¿’ä»»å‹™
```python
def execute_ml_task(self, task):
    """åŸ·è¡Œæ©Ÿå™¨å­¸ç¿’ä»»å‹™"""
    model_type = task['model_type']
    training_data = task['training_data']
    
    if model_type == 'tensorflow':
        return self.execute_tensorflow_task(task)
    elif model_type == 'pytorch':
        return self.execute_pytorch_task(task)
    elif model_type == 'scikit-learn':
        return self.execute_sklearn_task(task)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {model_type}")
```

#### æ•¸æ“šè™•ç†ä»»å‹™
```python
def execute_data_processing_task(self, task):
    """åŸ·è¡Œæ•¸æ“šè™•ç†ä»»å‹™"""
    processing_type = task['processing_type']
    input_data = task['input_data']
    
    if processing_type == 'csv_analysis':
        return self.execute_csv_analysis(task)
    elif processing_type == 'image_processing':
        return self.execute_image_processing(task)
    elif processing_type == 'text_analysis':
        return self.execute_text_analysis(task)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„è™•ç†é¡å‹: {processing_type}")
```

## ğŸ“Š è³‡æºç›£æ§

### ç³»çµ±è³‡æºç›£æ§
```python
import psutil
import time
import threading

class ResourceMonitor:
    def __init__(self, update_interval=5):
        self.update_interval = update_interval
        self.monitoring = False
        self.metrics = {}
        
    def start_monitoring(self):
        """é–‹å§‹è³‡æºç›£æ§"""
        self.monitoring = True
        monitor_thread = threading.Thread(target=self._monitor_loop)
        monitor_thread.daemon = True
        monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢è³‡æºç›£æ§"""
        self.monitoring = False
    
    def _monitor_loop(self):
        """ç›£æ§ä¸»å¾ªç’°"""
        while self.monitoring:
            self.metrics = self.collect_metrics()
            time.sleep(self.update_interval)
    
    def collect_metrics(self):
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        return {
            'timestamp': time.time(),
            'cpu': {
                'percent': psutil.cpu_percent(interval=1),
                'count': psutil.cpu_count(),
                'freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
            },
            'memory': psutil.virtual_memory()._asdict(),
            'disk': psutil.disk_usage('/')._asdict(),
            'network': psutil.net_io_counters()._asdict(),
            'processes': len(psutil.pids()),
            'load_avg': os.getloadavg() if hasattr(os, 'getloadavg') else None
        }
    
    def get_current_metrics(self):
        """ç²å–ç•¶å‰æŒ‡æ¨™"""
        return self.metrics.copy()
    
    def is_resource_available(self, required_resources):
        """æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„è³‡æºåŸ·è¡Œä»»å‹™"""
        current = self.get_current_metrics()
        
        # æª¢æŸ¥ CPU
        if 'cpu_cores' in required_resources:
            if current['cpu']['percent'] > 90:  # CPU ä½¿ç”¨ç‡éé«˜
                return False
        
        # æª¢æŸ¥è¨˜æ†¶é«”
        if 'memory_gb' in required_resources:
            required_memory = required_resources['memory_gb'] * 1024 * 1024 * 1024
            available_memory = current['memory']['available']
            if required_memory > available_memory:
                return False
        
        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        if 'disk_gb' in required_resources:
            required_disk = required_resources['disk_gb'] * 1024 * 1024 * 1024
            available_disk = current['disk']['free']
            if required_disk > available_disk:
                return False
        
        return True
```

### å®¹å™¨è³‡æºç›£æ§
```python
class ContainerMonitor:
    def __init__(self, docker_client):
        self.docker_client = docker_client
        
    def monitor_container(self, container_id):
        """ç›£æ§æŒ‡å®šå®¹å™¨çš„è³‡æºä½¿ç”¨æƒ…æ³"""
        try:
            container = self.docker_client.containers.get(container_id)
            stats = container.stats(stream=False)
            
            return self.parse_container_stats(stats)
        except docker.errors.NotFound:
            return None
    
    def parse_container_stats(self, stats):
        """è§£æå®¹å™¨çµ±è¨ˆæ•¸æ“š"""
        # CPU ä½¿ç”¨ç‡è¨ˆç®—
        cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                   stats['precpu_stats']['cpu_usage']['total_usage']
        system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                      stats['precpu_stats']['system_cpu_usage']
        
        cpu_percent = 0.0
        if system_delta > 0:
            cpu_percent = (cpu_delta / system_delta) * len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100.0
        
        # è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
        memory_usage = stats['memory_stats']['usage']
        memory_limit = stats['memory_stats']['limit']
        memory_percent = (memory_usage / memory_limit) * 100.0
        
        # ç¶²è·¯ I/O
        network_rx = 0
        network_tx = 0
        for interface, data in stats['networks'].items():
            network_rx += data['rx_bytes']
            network_tx += data['tx_bytes']
        
        return {
            'cpu_percent': cpu_percent,
            'memory_usage': memory_usage,
            'memory_limit': memory_limit,
            'memory_percent': memory_percent,
            'network_rx': network_rx,
            'network_tx': network_tx,
            'timestamp': time.time()
        }
```

## ğŸ” æ—¥èªŒå’Œç›£æ§

### çµæ§‹åŒ–æ—¥èªŒ
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name, log_file=None):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # å‰µå»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ§åˆ¶å°è™•ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶è™•ç†å™¨
        if log_file:
            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def log_task_event(self, event_type, task_id, details=None):
        """è¨˜éŒ„ä»»å‹™ç›¸é—œäº‹ä»¶"""
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'task_id': task_id,
            'node_id': self.node_id,
            'details': details or {}
        }
        
        self.logger.info(json.dumps(log_data))
    
    def log_system_event(self, event_type, details=None):
        """è¨˜éŒ„ç³»çµ±ç›¸é—œäº‹ä»¶"""
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'node_id': self.node_id,
            'details': details or {}
        }
        
        self.logger.info(json.dumps(log_data))
```

### å¥åº·æª¢æŸ¥
```python
class HealthChecker:
    def __init__(self, worker_node):
        self.worker_node = worker_node
        
    def check_health(self):
        """åŸ·è¡Œå¥åº·æª¢æŸ¥"""
        health_status = {
            'status': 'healthy',
            'timestamp': time.time(),
            'checks': {}
        }
        
        # æª¢æŸ¥ Node Pool é€£æ¥
        health_status['checks']['node_pool'] = self._check_node_pool_connection()
        
        # æª¢æŸ¥ Docker æœå‹™
        health_status['checks']['docker'] = self._check_docker_service()
        
        # æª¢æŸ¥ç³»çµ±è³‡æº
        health_status['checks']['resources'] = self._check_system_resources()
        
        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        health_status['checks']['disk_space'] = self._check_disk_space()
        
        # åˆ¤æ–·æ•´é«”ç‹€æ…‹
        if any(check['status'] == 'unhealthy' for check in health_status['checks'].values()):
            health_status['status'] = 'unhealthy'
        
        return health_status
    
    def _check_node_pool_connection(self):
        """æª¢æŸ¥èˆ‡ Node Pool çš„é€£æ¥"""
        try:
            self.worker_node.node_pool_client.ping()
            return {'status': 'healthy', 'message': 'Node Pool connection OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Node Pool connection failed: {str(e)}'}
    
    def _check_docker_service(self):
        """æª¢æŸ¥ Docker æœå‹™"""
        try:
            docker_client = docker.from_env()
            docker_client.ping()
            return {'status': 'healthy', 'message': 'Docker service OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Docker service failed: {str(e)}'}
    
    def _check_system_resources(self):
        """æª¢æŸ¥ç³»çµ±è³‡æº"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            if cpu_percent > 95:
                return {'status': 'unhealthy', 'message': f'High CPU usage: {cpu_percent}%'}
            
            if memory.percent > 95:
                return {'status': 'unhealthy', 'message': f'High memory usage: {memory.percent}%'}
            
            return {'status': 'healthy', 'message': 'System resources OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Resource check failed: {str(e)}'}
    
    def _check_disk_space(self):
        """æª¢æŸ¥ç£ç¢Ÿç©ºé–“"""
        try:
            disk = psutil.disk_usage('/')
            if disk.percent > 90:
                return {'status': 'unhealthy', 'message': f'Low disk space: {disk.percent}% used'}
            
            return {'status': 'healthy', 'message': 'Disk space OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Disk check failed: {str(e)}'}
```

## ğŸ”§ å¸¸è¦‹å•é¡Œæ’é™¤

### 1. Docker å®¹å™¨åŸ·è¡Œå¤±æ•—
**å•é¡Œ**: å®¹å™¨å•Ÿå‹•æˆ–åŸ·è¡Œå¤±æ•—
**è§£æ±º**:
```bash
# æª¢æŸ¥ Docker æœå‹™ç‹€æ…‹
sudo systemctl status docker

# æª¢æŸ¥ Docker æ˜ åƒæ˜¯å¦å­˜åœ¨
docker images

# æª¢æŸ¥å®¹å™¨æ—¥èªŒ
docker logs <container_id>

# æ¸…ç†åœæ­¢çš„å®¹å™¨
docker container prune
```

### 2. Node Pool é€£æ¥å•é¡Œ
**å•é¡Œ**: ç„¡æ³•é€£æ¥åˆ° Node Pool æœå‹™
**è§£æ±º**:
```python
# å¯¦æ–½é€£æ¥é‡è©¦æ©Ÿåˆ¶
import time
import grpc

def connect_with_retry(address, max_retries=5, retry_delay=2):
    for attempt in range(max_retries):
        try:
            channel = grpc.insecure_channel(address)
            # æ¸¬è©¦é€£æ¥
            grpc.channel_ready_future(channel).result(timeout=10)
            return channel
        except grpc.FutureTimeoutError:
            if attempt < max_retries - 1:
                print(f"é€£æ¥å¤±æ•—ï¼Œ{retry_delay}ç§’å¾Œé‡è©¦...")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•¸é€€é¿
            else:
                raise Exception("ç„¡æ³•é€£æ¥åˆ° Node Pool")
```

### 3. è³‡æºä¸è¶³å•é¡Œ
**å•é¡Œ**: ç³»çµ±è³‡æºä¸è¶³ç„¡æ³•åŸ·è¡Œä»»å‹™
**è§£æ±º**:
```python
# å¯¦æ–½è³‡æºæª¢æŸ¥å’Œç­‰å¾…æ©Ÿåˆ¶
def wait_for_resources(required_resources, timeout=300):
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        if resource_monitor.is_resource_available(required_resources):
            return True
        
        print("è³‡æºä¸è¶³ï¼Œç­‰å¾…ä¸­...")
        time.sleep(10)
    
    return False

# ä½¿ç”¨æ–¹å¼
if wait_for_resources({'memory_gb': 2, 'cpu_cores': 1}):
    execute_task(task)
else:
    report_task_failed(task_id, "è³‡æºä¸è¶³")
```

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–

### ä»»å‹™åŸ·è¡Œå„ªåŒ–
```python
# ä»»å‹™é è¼‰å…¥
class TaskPreloader:
    def __init__(self, docker_client):
        self.docker_client = docker_client
        self.preloaded_images = set()
    
    def preload_image(self, image_name):
        """é è¼‰å…¥ Docker æ˜ åƒ"""
        if image_name not in self.preloaded_images:
            try:
                self.docker_client.images.pull(image_name)
                self.preloaded_images.add(image_name)
                print(f"é è¼‰å…¥æ˜ åƒ: {image_name}")
            except Exception as e:
                print(f"é è¼‰å…¥æ˜ åƒå¤±æ•—: {e}")

# ä¸¦è¡Œä»»å‹™åŸ·è¡Œ
from concurrent.futures import ThreadPoolExecutor, as_completed

class ParallelTaskExecutor:
    def __init__(self, max_concurrent_tasks=2):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent_tasks)
        self.running_tasks = {}
    
    def submit_task(self, task):
        """æäº¤ä»»å‹™é€²è¡Œä¸¦è¡ŒåŸ·è¡Œ"""
        if len(self.running_tasks) < self.max_concurrent_tasks:
            future = self.executor.submit(self.execute_task, task)
            self.running_tasks[task['id']] = future
            return True
        return False
    
    def check_completed_tasks(self):
        """æª¢æŸ¥å·²å®Œæˆçš„ä»»å‹™"""
        completed_tasks = []
        
        for task_id, future in list(self.running_tasks.items()):
            if future.done():
                try:
                    result = future.result()
                    completed_tasks.append({'task_id': task_id, 'result': result})
                except Exception as e:
                    completed_tasks.append({'task_id': task_id, 'error': str(e)})
                
                del self.running_tasks[task_id]
        
        return completed_tasks
```

### è³‡æºä½¿ç”¨å„ªåŒ–
```python
# è¨˜æ†¶é«”ç®¡ç†
import gc

class MemoryManager:
    def __init__(self, memory_threshold=80):
        self.memory_threshold = memory_threshold
    
    def check_memory_usage(self):
        """æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        memory = psutil.virtual_memory()
        return memory.percent
    
    def cleanup_memory(self):
        """æ¸…ç†è¨˜æ†¶é«”"""
        gc.collect()
        
        # æ¸…ç† Docker æœªä½¿ç”¨çš„æ˜ åƒ
        docker_client = docker.from_env()
        docker_client.images.prune()
        
        # æ¸…ç†æœªä½¿ç”¨çš„å®¹å™¨
        docker_client.containers.prune()

# ç£ç¢Ÿç©ºé–“ç®¡ç†
class DiskManager:
    def __init__(self, cleanup_threshold=85):
        self.cleanup_threshold = cleanup_threshold
    
    def cleanup_old_files(self, directory, days=7):
        """æ¸…ç†èˆŠæ–‡ä»¶"""
        import os
        import time
        
        current_time = time.time()
        cutoff_time = current_time - (days * 24 * 60 * 60)
        
        for root, dirs, files in os.walk(directory):
            for file in files:
                file_path = os.path.join(root, file)
                if os.path.getmtime(file_path) < cutoff_time:
                    try:
                        os.remove(file_path)
                        print(f"åˆªé™¤èˆŠæ–‡ä»¶: {file_path}")
                    except Exception as e:
                        print(f"åˆªé™¤æ–‡ä»¶å¤±æ•—: {e}")
```

---

**ç›¸é—œæ–‡æª”**:
- [Node Pool æ¨¡çµ„](node-pool.md)
- [Master Node æ¨¡çµ„](master-node.md)
- [TaskWorker æ¨¡çµ„](taskworker.md)
- [API æ–‡æª”](../api.md)
- [éƒ¨ç½²æŒ‡å—](../deployment.md)
