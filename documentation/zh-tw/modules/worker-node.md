# Worker Node å·¥ä½œç¯€é»æ¨¡çµ„æ–‡æª”

HiveMind Worker Node æ˜¯åˆ†æ•£å¼è¨ˆç®—å¹³å°çš„æ ¸å¿ƒåŸ·è¡Œå–®å…ƒï¼Œæä¾›äº†ä¼æ¥­ç´šçš„ä»»å‹™åŸ·è¡Œã€è³‡æºç®¡ç†å’Œç›£æ§èƒ½åŠ›ã€‚

## ğŸ“‹ æ¦‚è¿°

Worker Node æ˜¯ä¸€å€‹å®Œæ•´çš„åˆ†æ•£å¼è¨ˆç®—åŸ·è¡Œç¯€é»ï¼Œå…·å‚™ä»¥ä¸‹ç‰¹æ€§ï¼š

### æ ¸å¿ƒç‰¹è‰²
- **ğŸ¯ å¤šä»»å‹™ä¸¦è¡ŒåŸ·è¡Œ**ï¼šæ”¯æ´åŒæ™‚åŸ·è¡Œå¤šå€‹è¨ˆç®—ä»»å‹™
- **ğŸ³ Docker å®¹å™¨åŒ–åŸ·è¡Œ**ï¼šå®‰å…¨éš”é›¢çš„ä»»å‹™åŸ·è¡Œç’°å¢ƒ
- **ğŸ”— è‡ªå‹• VPN é€£æ¥**ï¼šè‡ªå‹•é€£æ¥åˆ° HiveMind åˆ†æ•£å¼ç¶²è·¯
- **ğŸ“Š å³æ™‚è³‡æºç›£æ§**ï¼šCPUã€è¨˜æ†¶é«”ã€GPU å³æ™‚ç›£æ§
- **ğŸŒ Web ç®¡ç†ä»‹é¢**ï¼šç¾ä»£åŒ–çš„ç¯€é»ç®¡ç†ç•Œé¢
- **âš¡ ä¿¡ä»»è©•åˆ†ç³»çµ±**ï¼šåŸºæ–¼ç¯€é»è¡¨ç¾çš„å‹•æ…‹ä¿¡ä»»è©•åˆ†
- **ğŸ”’ ç”¨æˆ¶æœƒè©±ç®¡ç†**ï¼šå®‰å…¨çš„å¤šç”¨æˆ¶æœƒè©±æ”¯æ´

### æŠ€è¡“æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Worker Node Architecture             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Flask Web     â”‚    â”‚   gRPC Service  â”‚        â”‚
â”‚  â”‚   Interface     â”‚    â”‚   (Port 50053)  â”‚        â”‚
â”‚  â”‚  (Port 5000)    â”‚    â”‚                 â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                       â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                       â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚            Core WorkerNode Class                â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚ Multi-Task Execution Engine                 â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ running_tasks: Dict[task_id, task_info]   â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ task_locks: Dict[task_id, Lock]           â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ task_stop_events: Dict[task_id, Event]    â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚ Resource Management System                  â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ available_resources: CPU/Memory/GPU       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ total_resources: System capabilities      â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ resources_lock: Thread-safe access       â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚ Trust & Security System                     â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ trust_score: 0-100 reliability score     â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ trust_group: high/medium/low              â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ user_sessions: Multi-user support        â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚           â”‚                       â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Docker Engine  â”‚    â”‚ VPN Connection  â”‚        â”‚
â”‚  â”‚                 â”‚    â”‚ WireGuard Auto  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ æ–°æ¶æ§‹æª”æ¡ˆçµæ§‹

```
worker/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ hivemind_worker/           # ä¸»è¦ Python å¥—ä»¶
â”‚       â”œâ”€â”€ worker_node.py         # æ ¸å¿ƒå·¥ä½œç¯€é»å¯¦ç¾
â”‚       â”œâ”€â”€ nodepool_pb2.py        # gRPC Protocol Buffers
â”‚       â”œâ”€â”€ nodepool_pb2_grpc.py   # gRPC æœå‹™å®¢æˆ¶ç«¯
â”‚       â”œâ”€â”€ __init__.py            # å¥—ä»¶åˆå§‹åŒ–
â”‚       â”œâ”€â”€ static/                # Web ç•Œé¢éœæ…‹è³‡æº
â”‚       â”‚   â”œâ”€â”€ css/              # æ¨£å¼æ–‡ä»¶
â”‚       â”‚   â”œâ”€â”€ js/               # JavaScript æ–‡ä»¶
â”‚       â”‚   â””â”€â”€ images/           # åœ–ç‰‡è³‡æº
â”‚       â””â”€â”€ templates/             # Flask HTML æ¨¡æ¿
â”‚           â”œâ”€â”€ dashboard.html     # ä¸»å„€è¡¨æ¿
â”‚           â”œâ”€â”€ login.html         # ç™»å…¥é é¢
â”‚           â””â”€â”€ tasks.html         # ä»»å‹™ç®¡ç†é é¢
â”œâ”€â”€ main.py                        # ä¸»å…¥å£é»
â”œâ”€â”€ pyproject.toml                 # Python å°ˆæ¡ˆé…ç½®
â”œâ”€â”€ requirements.txt               # ä¾è³´å¥—ä»¶
â”œâ”€â”€ Dockerfile                     # Docker å»ºæ§‹æ–‡ä»¶
â”œâ”€â”€ run_task.sh                    # ä»»å‹™åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ nodepool.proto                 # gRPC æœå‹™å®šç¾©
â””â”€â”€ README.md                      # èªªæ˜æ–‡ä»¶
```

## ğŸ”§ æ ¸å¿ƒçµ„ä»¶è©³è§£

### 1. ä¸»è¦ WorkerNode é¡åˆ¥

```python
class WorkerNode:
    def __init__(self):
        # ç¯€é»åŸºæœ¬ä¿¡æ¯
        self.node_id = NODE_ID
        self.port = NODE_PORT
        self.master_address = MASTER_ADDRESS
        
        # å¤šä»»å‹™åŸ·è¡Œç³»çµ±
        self.running_tasks = {}     # {task_id: task_info}
        self.task_locks = {}        # {task_id: threading.Lock()}
        self.task_stop_events = {}  # {task_id: Event()}
        
        # è³‡æºç®¡ç†ç³»çµ±
        self.available_resources = {
            "cpu": 0,               # CPU åˆ†æ•¸
            "memory_gb": 0,         # å¯ç”¨è¨˜æ†¶é«” GB
            "gpu": 0,               # GPU åˆ†æ•¸  
            "gpu_memory_gb": 0      # GPU è¨˜æ†¶é«” GB
        }
        
        # ä¿¡ä»»èˆ‡å®‰å…¨ç³»çµ±
        self.trust_score = 0        # 0-100 ä¿¡ä»»åˆ†æ•¸
        self.trust_group = "low"    # high/medium/low
        self.user_sessions = {}     # å¤šç”¨æˆ¶æœƒè©±
```

### 2. å¤šä»»å‹™åŸ·è¡Œå¼•æ“

**ä¸¦è¡Œä»»å‹™æ”¯æ´**ï¼š
```python
def execute_task(self, task_id, task_data):
    """åŸ·è¡Œä»»å‹™ï¼ˆæ”¯æ´ä¸¦è¡Œï¼‰"""
    # 1. å‰µå»ºä»»å‹™å°ˆç”¨é–å’Œåœæ­¢äº‹ä»¶
    self.task_locks[task_id] = threading.Lock()
    self.task_stop_events[task_id] = threading.Event()
    
    # 2. åˆ†é…è³‡æº
    required_resources = self._calculate_task_resources(task_data)
    if not self._allocate_resources(task_id, required_resources):
        return {"error": "Insufficient resources"}
    
    # 3. åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œä»»å‹™
    task_thread = threading.Thread(
        target=self._run_task_in_thread,
        args=(task_id, task_data),
        name=f"Task-{task_id}"
    )
    task_thread.start()
    
    return {"status": "started", "task_id": task_id}

def _run_task_in_thread(self, task_id, task_data):
    """åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œä»»å‹™"""
    try:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºé‹è¡Œä¸­
        with self.task_locks[task_id]:
            self.running_tasks[task_id] = {
                "status": "RUNNING",
                "start_time": time(),
                "resources": task_data.get("required_resources", {})
            }
        
        # åŸ·è¡Œ Docker å®¹å™¨ä»»å‹™
        result = self._execute_docker_task(task_id, task_data)
        
        # æ›´æ–°å®Œæˆç‹€æ…‹
        with self.task_locks[task_id]:
            self.running_tasks[task_id]["status"] = "COMPLETED"
            self.running_tasks[task_id]["result"] = result
            
    except Exception as e:
        with self.task_locks[task_id]:
            self.running_tasks[task_id]["status"] = "FAILED"
            self.running_tasks[task_id]["error"] = str(e)
    finally:
        # é‡‹æ”¾è³‡æº
        self._release_task_resources(task_id)
```

### 3. å‹•æ…‹è³‡æºç®¡ç†

**æ™ºèƒ½è³‡æºåˆ†é…**ï¼š
```python
def _allocate_resources(self, task_id, required_resources):
    """ç‚ºä»»å‹™åˆ†é…è³‡æº"""
    with self.resources_lock:
        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ è³‡æº
        if (self.available_resources["cpu"] >= required_resources.get("cpu", 0) and
            self.available_resources["memory_gb"] >= required_resources.get("memory_gb", 0) and
            self.available_resources["gpu"] >= required_resources.get("gpu", 0)):
            
            # åˆ†é…è³‡æº
            for resource, amount in required_resources.items():
                self.available_resources[resource] -= amount
            
            # è¨˜éŒ„ä»»å‹™è³‡æºä½¿ç”¨
            self.running_tasks[task_id] = {
                "status": "ALLOCATED",
                "resources": required_resources,
                "start_time": time()
            }
            return True
        
        return False

def _release_task_resources(self, task_id):
    """é‡‹æ”¾ä»»å‹™è³‡æº"""
    with self.resources_lock:
        if task_id in self.running_tasks:
            task_resources = self.running_tasks[task_id].get("resources", {})
            
            # æ­¸é‚„è³‡æº
            for resource, amount in task_resources.items():
                if resource in self.available_resources:
                    self.available_resources[resource] += amount
                    # ç¢ºä¿ä¸è¶…éç¸½è³‡æºé™åˆ¶
                    self.available_resources[resource] = min(
                        self.available_resources[resource],
                        self.total_resources[resource]
                    )
```

### 4. VPN è‡ªå‹•é€£æ¥ç³»çµ±

**è‡ªå‹•ç¶²è·¯åŠ å…¥**ï¼š
```python
def _auto_join_vpn(self):
    """è‡ªå‹•é€£æ¥åˆ° HiveMind VPN ç¶²è·¯"""
    try:
        # æª¢æŸ¥æ˜¯å¦å·²é€£æ¥åˆ° VPN
        if self._check_vpn_connection():
            self._log("Already connected to HiveMind network")
            return
        
        # å¼•å°ç”¨æˆ¶æ‰‹å‹•é€£æ¥ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰
        self._log("VPN connection required for HiveMind network")
        self._log("Please ensure WireGuard VPN is configured and running")
        
        # ç­‰å¾…ç”¨æˆ¶ç¢ºèªé€£æ¥
        print("å¦‚æœæ‚¨å·²ç¶“é€£æ¥ï¼Œè«‹æŒ‰ y")
        response = input()
        if response.lower() == 'y':
            self._log("VPN connection confirmed")
        
    except Exception as e:
        self._log(f"VPN setup failed: {e}", WARNING)

def _get_local_ip(self):
    """ç²å–æœ¬æ©Ÿ IPï¼ˆå„ªå…ˆ WireGuard ç¶²å¡ï¼‰"""
    try:
        interfaces_list = interfaces()
        
        # å„ªå…ˆæª¢æŸ¥ WireGuard ç¶²å¡
        wg_interfaces = [iface for iface in interfaces_list 
                        if 'wg' in iface.lower() or 'wireguard' in iface.lower()]
        
        if wg_interfaces:
            for wg_iface in wg_interfaces:
                addrs = ifaddresses(wg_iface)
                if AF_INET in addrs:
                    return addrs[AF_INET][0]['addr']
        
        # æª¢æŸ¥ 10.0.0.x VPN ç¶²æ®µ
        for iface in interfaces_list:
            addrs = ifaddresses(iface)
            if AF_INET in addrs:
                for addr_info in addrs[AF_INET]:
                    ip = addr_info['addr']
                    if ip.startswith('10.0.0.') and ip != '10.0.0.1':
                        return ip
        
        return '127.0.0.1'
        
    except Exception as e:
        self._log(f"IP detection failed: {e}")
        return '127.0.0.1'
```

## ğŸŒ Web ç®¡ç†ä»‹é¢

Worker Node æä¾›äº†ç¾ä»£åŒ–çš„ Web ç®¡ç†ä»‹é¢ï¼Œæ”¯æ´ï¼š

### ä¸»è¦åŠŸèƒ½é é¢

1. **å„€è¡¨æ¿ (`/dashboard`)**
   - ç¯€é»å³æ™‚ç‹€æ…‹ç›£æ§
   - è³‡æºä½¿ç”¨ç‡åœ–è¡¨
   - ä»»å‹™åŸ·è¡Œçµ±è¨ˆ
   - ç³»çµ±æ•ˆèƒ½æŒ‡æ¨™

2. **ä»»å‹™ç®¡ç† (`/tasks`)**
   - ç•¶å‰é‹è¡Œä»»å‹™åˆ—è¡¨
   - ä»»å‹™æ­·å²è¨˜éŒ„
   - ä»»å‹™è©³ç´°ä¿¡æ¯
   - ä»»å‹™åœæ­¢æ§åˆ¶

3. **ç³»çµ±ç›£æ§ (`/monitor`)**
   - CPU ä½¿ç”¨ç‡
   - è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³
   - Docker ç‹€æ…‹
   - ç¶²è·¯é€£æ¥ç‹€æ…‹

### RESTful API ç«¯é»

```python
# ç³»çµ±ç‹€æ…‹ API
@app.route('/api/status')
def api_status():
    """ç²å–ç¯€é»å®Œæ•´ç‹€æ…‹ä¿¡æ¯"""
    return jsonify({
        'node_id': self.node_id,
        'status': self.status,
        'is_registered': self.is_registered,
        'docker_available': self.docker_available,
        'cpu_percent': cpu_percent(),
        'memory_percent': virtual_memory().percent,
        'available_resources': self.available_resources,
        'total_resources': self.total_resources,
        'running_tasks': len(self.running_tasks),
        'trust_score': self.trust_score,
        'trust_group': self.trust_group
    })

# ä»»å‹™ç®¡ç† API
@app.route('/api/tasks')
def api_tasks():
    """ç²å–æ‰€æœ‰é‹è¡Œä¸­çš„ä»»å‹™"""
    tasks_info = []
    with self.resources_lock:
        for task_id, task_data in self.running_tasks.items():
            tasks_info.append({
                'task_id': task_id,
                'status': task_data.get('status', 'Unknown'),
                'start_time': datetime.fromtimestamp(
                    task_data.get('start_time', 0)
                ).isoformat(),
                'elapsed': time() - task_data.get('start_time', time()),
                'resources': task_data.get('resources', {})
            })
    return jsonify({'tasks': tasks_info})

# ä»»å‹™æ§åˆ¶ API
@app.route('/api/stop_task/<task_id>', methods=['POST'])
def api_stop_task(task_id):
    """åœæ­¢æŒ‡å®šä»»å‹™"""
    if task_id in self.task_stop_events:
        self.task_stop_events[task_id].set()
        return jsonify({'success': True, 'message': f'Task {task_id} stopped'})
    return jsonify({'success': False, 'error': 'Task not found'}), 404
```

## ğŸ³ Docker å®¹å™¨ç®¡ç†

### å®¹å™¨åŒ–ä»»å‹™åŸ·è¡Œ

```python
def _execute_docker_task(self, task_id, task_data):
    """åœ¨ Docker å®¹å™¨ä¸­åŸ·è¡Œä»»å‹™"""
    try:
        # 1. æº–å‚™å·¥ä½œç›®éŒ„
        work_dir = self._prepare_task_workspace(task_id, task_data)
        
        # 2. å‰µå»º Docker å®¹å™¨
        container = self.docker_client.containers.run(
            image=task_data.get('docker_image', 'justin308/hivemind-worker:latest'),
            command=task_data.get('command', 'python task.py'),
            name=f"task-{task_id}",
            volumes={work_dir: {'bind': '/workspace', 'mode': 'rw'}},
            working_dir='/workspace',
            mem_limit=task_data.get('memory_limit', '1g'),
            cpu_quota=task_data.get('cpu_quota', 100000),
            detach=True,
            remove=False,
            network_mode='bridge',
            environment=task_data.get('environment', {})
        )
        
        # 3. ç›£æ§å®¹å™¨åŸ·è¡Œ
        while container.status != 'exited':
            # æª¢æŸ¥åœæ­¢ä¿¡è™Ÿ
            if self.task_stop_events[task_id].is_set():
                container.stop(timeout=10)
                break
                
            sleep(1)
            container.reload()
        
        # 4. æ”¶é›†åŸ·è¡Œçµæœ
        logs = container.logs(stdout=True, stderr=True).decode('utf-8')
        exit_code = container.attrs['State']['ExitCode']
        
        # 5. æ”¶é›†è¼¸å‡ºæ–‡ä»¶
        result_files = self._collect_task_results(container, work_dir)
        
        # 6. æ¸…ç†å®¹å™¨
        container.remove()
        
        return {
            'exit_code': exit_code,
            'logs': logs,
            'files': result_files,
            'status': 'completed' if exit_code == 0 else 'failed'
        }
        
    except Exception as e:
        self._log(f"Docker task execution failed: {e}", ERROR)
        return {'error': str(e), 'status': 'failed'}
```

## ğŸ“Š ä¿¡ä»»è©•åˆ†ç³»çµ±

Worker Node å¯¦ç¾äº†å‹•æ…‹ä¿¡ä»»è©•åˆ†æ©Ÿåˆ¶ï¼š

### ä¿¡ä»»åˆ†æ•¸è¨ˆç®—

```python
def update_trust_score(self, task_result):
    """æ›´æ–°ç¯€é»ä¿¡ä»»åˆ†æ•¸"""
    if task_result.get('status') == 'completed':
        # æˆåŠŸå®Œæˆä»»å‹™æå‡ä¿¡ä»»åˆ†æ•¸
        self.trust_score = min(100, self.trust_score + 2)
    elif task_result.get('status') == 'failed':
        # ä»»å‹™å¤±æ•—é™ä½ä¿¡ä»»åˆ†æ•¸
        self.trust_score = max(0, self.trust_score - 5)
    
    # æ›´æ–°ä¿¡ä»»åˆ†çµ„
    if self.trust_score >= 80:
        self.trust_group = "high"
    elif self.trust_score >= 50:
        self.trust_group = "medium"
    else:
        self.trust_group = "low"
    
    self._log(f"Trust score updated: {self.trust_score} (Group: {self.trust_group})")
```

## ğŸš€ éƒ¨ç½²å’Œé…ç½®

### ç’°å¢ƒè®Šæ•¸é…ç½®

```bash
# åŸºæœ¬é…ç½®
NODE_PORT=50053                    # gRPC æœå‹™ç«¯å£
FLASK_PORT=5000                    # Web ç•Œé¢ç«¯å£
MASTER_ADDRESS=10.0.0.1:50051     # Master Node åœ°å€
NODE_ID=worker-hostname-50053      # ç¯€é»å”¯ä¸€è­˜åˆ¥

# Docker é…ç½®
DOCKER_ENABLED=true                # å•Ÿç”¨ Docker æ”¯æ´
DOCKER_NETWORK=hivemind           # Docker ç¶²è·¯åç¨±

# è³‡æºé…ç½®
MAX_CPU_CORES=4                   # æœ€å¤§ CPU æ ¸å¿ƒæ•¸
MAX_MEMORY_GB=8                   # æœ€å¤§è¨˜æ†¶é«” GB
MAX_CONCURRENT_TASKS=3            # æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸

# å®‰å…¨é…ç½®
SESSION_TIMEOUT=24                # æœƒè©±è¶…æ™‚æ™‚é–“ï¼ˆå°æ™‚ï¼‰
TRUST_THRESHOLD=50               # ä¿¡ä»»åˆ†æ•¸é–¾å€¼
```

### ä½¿ç”¨ Python å¥—ä»¶å®‰è£

```bash
# 1. å¾åŸå§‹ç¢¼å®‰è£
cd worker/
pip install -e .

# 2. ä½¿ç”¨é å»ºå¥—ä»¶
pip install hivemind_worker==0.0.7

# 3. åŸ·è¡Œ Worker Node
python -m hivemind_worker.worker_node
```

### Docker éƒ¨ç½²

```bash
# 1. å»ºæ§‹ Docker æ˜ åƒ
docker build -t hivemind-worker:latest .

# 2. åŸ·è¡Œå®¹å™¨
docker run -d \
  --name hivemind-worker \
  --network host \
  -e MASTER_ADDRESS=10.0.0.1:50051 \
  -e NODE_PORT=50053 \
  -e FLASK_PORT=5000 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  hivemind-worker:latest
```

## ğŸ” ç›£æ§å’Œæ—¥èªŒ

### æ—¥èªŒç³»çµ±

Worker Node æä¾›å®Œæ•´çš„æ—¥èªŒè¨˜éŒ„ï¼š

```python
def _log(self, message, level=INFO):
    """ç·šç¨‹å®‰å…¨çš„æ—¥èªŒè¨˜éŒ„"""
    with self.log_lock:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] {getLevelName(level)}: {message}"
        
        # æ·»åŠ åˆ°å…§å­˜æ—¥èªŒ
        self.logs.append({
            'timestamp': timestamp,
            'level': getLevelName(level),
            'message': message
        })
        
        # ä¿æŒæ—¥èªŒå¤§å°é™åˆ¶
        if len(self.logs) > 1000:
            self.logs = self.logs[-500:]  # ä¿ç•™æœ€æ–° 500 æ¢
        
        print(log_entry)
```

### å¥åº·æª¢æŸ¥

```python
@app.route('/health')
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    checks = {
        'docker': self._check_docker_health(),
        'grpc': self._check_grpc_connection(),
        'resources': self._check_resource_availability(),
        'vpn': self._check_vpn_connection()
    }
    
    all_healthy = all(check['status'] == 'healthy' for check in checks.values())
    
    return jsonify({
        'status': 'healthy' if all_healthy else 'unhealthy',
        'checks': checks,
        'timestamp': datetime.now().isoformat()
    }), 200 if all_healthy else 503
```

## ğŸ”§ å¸¸è¦‹å•é¡Œæ’è§£

### 1. Docker é€£æ¥å•é¡Œ

```bash
# æª¢æŸ¥ Docker æœå‹™ç‹€æ…‹
systemctl status docker

# é‡å•Ÿ Docker æœå‹™
sudo systemctl restart docker

# æª¢æŸ¥ Docker æ¬Šé™
sudo usermod -aG docker $USER
```

### 2. VPN é€£æ¥å•é¡Œ

```bash
# æª¢æŸ¥ WireGuard ç‹€æ…‹
sudo wg show

# é‡å•Ÿ WireGuard
sudo systemctl restart wg-quick@wg0

# æª¢æŸ¥è·¯ç”±è¡¨
ip route | grep 10.0.0
```

### 3. è³‡æºä¸è¶³å•é¡Œ

```python
# æª¢æŸ¥ç³»çµ±è³‡æº
@app.route('/api/resources')
def check_resources():
    return jsonify({
        'cpu_available': psutil.cpu_percent(interval=1),
        'memory_available': psutil.virtual_memory().available / (1024**3),
        'disk_available': psutil.disk_usage('/').free / (1024**3),
        'running_tasks': len(self.running_tasks)
    })
```

### 4. ä»»å‹™åŸ·è¡Œå¤±æ•—

æª¢æŸ¥ä»»å‹™æ—¥èªŒï¼š
```bash
# æª¢æŸ¥å®¹å™¨æ—¥èªŒ
docker logs task-{task_id}

# æª¢æŸ¥å·¥ä½œç›®éŒ„
ls -la /tmp/hivemind/tasks/{task_id}/
```

## ğŸ“ˆ æ•ˆèƒ½å„ªåŒ–

### 1. è³‡æºèª¿åº¦å„ªåŒ–

```python
def _optimize_resource_allocation(self):
    """å„ªåŒ–è³‡æºåˆ†é…ç­–ç•¥"""
    # æ ¹æ“šæ­·å²æ•¸æ“šèª¿æ•´è³‡æºåˆ†é…
    cpu_utilization = self._get_average_cpu_usage()
    memory_utilization = self._get_average_memory_usage()
    
    # å‹•æ…‹èª¿æ•´å¯ç”¨è³‡æº
    if cpu_utilization < 0.7:
        self.available_resources['cpu'] = min(
            self.total_resources['cpu'],
            self.available_resources['cpu'] * 1.1
        )
```

### 2. ç¶²è·¯å„ªåŒ–

```python
def _optimize_network_settings(self):
    """å„ªåŒ–ç¶²è·¯è¨­å®š"""
    # èª¿æ•´ gRPC é€£æ¥åƒæ•¸
    options = [
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 10000),
        ('grpc.keepalive_permit_without_calls', True),
        ('grpc.http2.max_pings_without_data', 0),
        ('grpc.http2.min_time_between_pings_ms', 10000),
        ('grpc.http2.min_ping_interval_without_data_ms', 300000)
    ]
    return options
```

é€™å€‹æ›´æ–°çš„ Worker Node æ–‡æª”åæ˜ äº†æœ€æ–°çš„æ¶æ§‹æ”¹é€²ï¼ŒåŒ…æ‹¬å¤šä»»å‹™æ”¯æ´ã€ä¿¡ä»»è©•åˆ†ç³»çµ±ã€VPN è‡ªå‹•é€£æ¥å’Œç¾ä»£åŒ–çš„ Web ä»‹é¢ã€‚
